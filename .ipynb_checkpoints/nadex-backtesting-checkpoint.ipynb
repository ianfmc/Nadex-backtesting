{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97bdc6e-d7d2-4caf-813d-c5f641c1b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pyathena in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.15.0)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyathena) (1.38.38)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyathena) (1.38.38)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyathena) (2025.5.1)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyathena) (2.9.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyathena) (9.1.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3>=1.26.4->pyathena) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3>=1.26.4->pyathena) (0.13.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore>=1.29.4->pyathena) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil->pyathena) (1.17.0)\n",
      "Requirement already satisfied: yfinance in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.2.65)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (2.32.4)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (4.25.8)\n",
      "Requirement already satisfied: websockets>=13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.0)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from curl_cffi>=0.7->yfinance) (2025.6.15)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests>=2.31->yfinance) (1.26.19)\n",
      "\u001b[33mWARNING: Skipping pypdf as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: fpdf2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.8.3)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fpdf2) (11.2.1)\n",
      "Requirement already satisfied: fonttools>=4.34.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fpdf2) (4.58.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: numexpr in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from numexpr) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyathena\n",
    "\n",
    "!pip install yfinance\n",
    "!pip uninstall --yes pypdf\n",
    "!pip install --upgrade fpdf2\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install --upgrade numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbdd23-d521-4f89-bb86-7d0eb238380e",
   "metadata": {},
   "source": [
    "**Define a class to manage the technical indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82652199-0044-4aae-b59d-6a8a1b09430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TechnicalFactors:\n",
    "    # EMA crossover lengths\n",
    "    ema_short: int = 12\n",
    "    ema_long: int = 26\n",
    "\n",
    "    # MACD periods (fast, slow, signal)\n",
    "    macd_fast: int = 12\n",
    "    macd_slow: int = 26\n",
    "    macd_signal: int = 9\n",
    "\n",
    "    # RSI lookback and decision threshold\n",
    "    rsi_period: int = 14\n",
    "    rsi_threshold: float = 50.0\n",
    "\n",
    "    # ATR lookback and how wide your strike buffer is\n",
    "    atr_period: int = 14\n",
    "    volatility_multiplier: float = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ef1c3-5ce8-43ce-a960-0800d7497d1a",
   "metadata": {},
   "source": [
    "**Define a function to parse the expiration date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5119c7-4ec6-439f-9817-2efcb594afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "def parse_expiration(\n",
    "    contract_date: date,\n",
    "    exp_time_str: str,\n",
    ") -> datetime:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - contract_date: a datetime.date (e.g. 2025-07-18)\n",
    "      - exp_time_str : a string like \"07/17/2025 11:00 pm\"\n",
    "    Return a datetime for the full expiration.\n",
    "    \"\"\"\n",
    "    # Parse the MM/DD/YYYY hh:mm am/pm pattern\n",
    "    return datetime.strptime(exp_time_str, \"%m/%d/%Y %I:%M %p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46764678-45ab-4904-8c71-e629a115111d",
   "metadata": {},
   "source": [
    "**Define a function to get the historical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a579eaae-706e-473d-8fe1-9b05350a589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def load_recent_history(\n",
    "    s3_client,\n",
    "    bucket: str,\n",
    "    prefix: str,\n",
    "    days: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the last `days` of tradingResults CSVs from S3 into one DataFrame.\n",
    "    Assumes each key under `prefix` is named <YYYYMMDD>_tradingResults.csv.\n",
    "    Picks up Date, Name, Exp Time, Strike Price, In the Money, Ticker.\n",
    "    \"\"\"\n",
    "    cutoff = datetime.utcnow().date() - timedelta(days=days)\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    dfs = []\n",
    "\n",
    "    # Define all 7 raw columns, then pick a subset\n",
    "    all_cols = [\n",
    "        \"date\",        # Date\n",
    "        \"name\",        # Name\n",
    "        \"exp_time\",    # Exp Time\n",
    "        \"_exp_value\",  # Exp Value (not used)\n",
    "        \"strike\",      # Strike Price\n",
    "        \"in_the_money\",# In the Money\n",
    "        \"ticker\",      # Ticker\n",
    "    ]\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\"):\n",
    "                continue\n",
    "\n",
    "            # Derive partition date from filename\n",
    "            file_dt = key.split(\"/\")[-1].split(\"_\", 1)[0]\n",
    "            dt = datetime.strptime(file_dt, \"%Y%m%d\").date()\n",
    "            if dt < cutoff:\n",
    "                continue\n",
    "\n",
    "            body = s3_client.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "            df = pd.read_csv(\n",
    "                io.BytesIO(body),\n",
    "                skiprows=1,                    # skip the header row\n",
    "                names=all_cols,\n",
    "                usecols=[\"date\",\"name\",\"exp_time\",\"strike\",\"in_the_money\",\"ticker\"],\n",
    "                dtype={\"in_the_money\": \"Int64\"},\n",
    "                na_filter=False,\n",
    "            )\n",
    "            # Parse the Date column into a date object\n",
    "            df[\"dt\"] = pd.to_datetime(df[\"date\"], format=\"%d-%b-%y\").dt.date\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"date\",\"name\",\"exp_time\",\"strike\",\"in_the_money\",\"ticker\",\"dt\"])\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def write_to_s3(\n",
    "    df: pd.DataFrame,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Write DataFrame to S3 (CSV) and return it, so it\n",
    "    can stay inside a .pipe() chain.\n",
    "    \"\"\"\n",
    "    path = f\"s3://{bucket}/{key}\"\n",
    "    df.to_csv(path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5eac1-ad90-42eb-b015-3794fa848572",
   "metadata": {},
   "source": [
    "**Define a function to load the pricing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f4e4fa-6b64-4fcc-a855-a0b04d2a2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from typing import Union\n",
    "from datetime import datetime\n",
    "\n",
    "DateLike = Union[str, datetime]\n",
    "\n",
    "def fetch_price(\n",
    "    ticker: str,\n",
    "    start: DateLike,\n",
    "    end: DateLike,\n",
    "    interval: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download OHLCV for `ticker` between `start` and `end` at `interval`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ticker : str\n",
    "        e.g. \"AAPL\"\n",
    "    start : str or datetime\n",
    "        Inclusive start date, e.g. \"2025-01-01\"\n",
    "    end : str or datetime\n",
    "        Exclusive end date, e.g. \"2025-07-14\"\n",
    "    interval : str\n",
    "        e.g. \"1d\", \"1h\", \"5m\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: [Date, Open, High, Low, Close, Adj Close, Volume, ticker]\n",
    "    \"\"\"\n",
    "    df = yf.download(\n",
    "        tickers=ticker,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        interval=interval,\n",
    "        auto_adjust=True,\n",
    "        progress=False\n",
    "    )\n",
    "    # Drop multi‐level column if present (yfinance sometimes nests tickers)\n",
    "    df.columns = df.columns.get_level_values(0)  \n",
    "    df = df.reset_index()\n",
    "    df.columns.name = None\n",
    "    df[\"ticker\"] = ticker\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b36a5f-5c26-4d66-85a4-7c1bd5ca22f7",
   "metadata": {},
   "source": [
    "**Define a function to compute the technical indicators**\n",
    "1. Compute the MACD\n",
    "2. Compute the RSI\n",
    "3. Compute the ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce47bac9-4a2f-48a2-a326-294b420fc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_macd(df: pd.DataFrame, factors: TechnicalFactors) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute EMA{short}, EMA{long}, MACD, Signal, and MACD_hist\n",
    "    using the spans defined in factors.\n",
    "    \"\"\"\n",
    "    price = df[\"Close\"]\n",
    "    ema_short = price.ewm(span=factors.ema_short, adjust=False).mean()\n",
    "    ema_long  = price.ewm(span=factors.ema_long,  adjust=False).mean()\n",
    "    macd      = ema_short - ema_long\n",
    "    signal    = macd.ewm(span=factors.macd_signal, adjust=False).mean()\n",
    "    hist      = macd - signal\n",
    "\n",
    "    return df.assign(\n",
    "        **{\n",
    "            f\"EMA{factors.ema_short}\": ema_short,\n",
    "            f\"EMA{factors.ema_long}\":  ema_long,\n",
    "            \"MACD\":       macd,\n",
    "            \"Signal\":     signal,\n",
    "            \"MACD_hist\":  hist,\n",
    "        }\n",
    "    )\n",
    "\n",
    "def compute_rsi(df: pd.DataFrame, factors: TechnicalFactors) -> pd.DataFrame:\n",
    "    \"\"\"Compute RSI over factors.rsi_period.\"\"\"\n",
    "    delta = df[\"Close\"].diff()\n",
    "    gain  = delta.clip(lower=0)\n",
    "    loss  = -delta.clip(upper=0)\n",
    "    avg_gain = gain.ewm(com=factors.rsi_period - 1, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(com=factors.rsi_period - 1, adjust=False).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return df.assign(RSI=rsi)\n",
    "\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, factors: TechnicalFactors) -> pd.DataFrame:\n",
    "    \"\"\"Compute ATR over factors.atr_period.\"\"\"\n",
    "    high_low = df[\"High\"] - df[\"Low\"]\n",
    "    high_pc  = (df[\"High\"] - df[\"Close\"].shift(1)).abs()\n",
    "    low_pc   = (df[\"Low\"]  - df[\"Close\"].shift(1)).abs()\n",
    "    tr = pd.concat([high_low, high_pc, low_pc], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(window=factors.atr_period).mean()\n",
    "    return df.assign(ATR=atr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f963bb0-9745-439e-88cc-05fd415ac24e",
   "metadata": {},
   "source": [
    "*Define a function to determine the trading signal**\n",
    "1. Determine the Trend\n",
    "2. Check the Momentum\n",
    "3. Determine the Trigger\n",
    "4. Check the Volatility\n",
    "5. Generate the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67546cc7-8da5-4357-874f-30ca7a0ff7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def determine_trend(row: pd.Series, factors: TechnicalFactors):\n",
    "    short = row[f\"EMA{factors.ema_short}\"]\n",
    "    long_ = row[f\"EMA{factors.ema_long}\"]\n",
    "    diff  = short - long_\n",
    "\n",
    "    if (short > long_) and (row.Close > short) and (row.Close > long_):\n",
    "        return \"up\", diff\n",
    "    if (short < long_) and (row.Close < short) and (row.Close < long_):\n",
    "        return \"down\", diff\n",
    "    return \"sideways\", diff\n",
    "\n",
    "\n",
    "def momentum_check(row: pd.Series, trend: str, factors: TechnicalFactors):\n",
    "    rsi = row.RSI\n",
    "    macd_diff = row.MACD - row.Signal\n",
    "    thr = factors.rsi_threshold\n",
    "\n",
    "    if trend == \"up\":\n",
    "        ok = (rsi > thr) and (macd_diff > 0)\n",
    "    elif trend == \"down\":\n",
    "        ok = (rsi < thr) and (macd_diff < 0)\n",
    "    else:\n",
    "        ok = False\n",
    "    return ok, rsi\n",
    "\n",
    "def volatility_check(row: pd.Series, strike_diff: float, factors: TechnicalFactors):\n",
    "    ok = strike_diff <= factors.volatility_multiplier * row.ATR\n",
    "    return ok, strike_diff   \n",
    "\n",
    "def signal_trigger(row: pd.Series, trend: str, factors: TechnicalFactors):\n",
    "    macd_diff = row.MACD - row.Signal\n",
    "    thr = factors.rsi_threshold\n",
    "\n",
    "    if trend == \"up\":\n",
    "        ok = (macd_diff > 0) and (row.RSI > thr)\n",
    "    elif trend == \"down\":\n",
    "        ok = (macd_diff < 0) and (row.RSI < thr)\n",
    "    else:\n",
    "        ok = False\n",
    "    return ok, macd_diff\n",
    "\n",
    "def generate_trade_signal(\n",
    "    df: pd.DataFrame,\n",
    "    asset_symbol: str,\n",
    "    strike_price: float,\n",
    "    factors: TechnicalFactors,\n",
    "    expiry: str = \"EOD\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Append:\n",
    "      - signal (1=Buy, 0=Sell, <NA>=No trade)\n",
    "      - recommendation (text)\n",
    "      - TrendValue, MomentumValue, SignalValue, VolatilityValue\n",
    "    \"\"\"\n",
    "    last = df.iloc[-1]\n",
    "    trend, trend_val       = determine_trend(last, factors)\n",
    "    mom_ok, mom_val        = momentum_check(last, trend, factors)\n",
    "    sig_ok, sig_val        = signal_trigger(last, trend, factors)\n",
    "    strike_diff            = abs(last.Close - strike_price)\n",
    "    vol_ok, vol_val        = volatility_check(last, strike_diff, factors)\n",
    "\n",
    "    # Decide Buy / Sell / No trade\n",
    "    if trend == \"up\" and mom_ok and sig_ok and vol_ok:\n",
    "        signal_val = 1\n",
    "        rec_text   = f\"Buy {asset_symbol} @ {strike_price:.4f} ({expiry})\"\n",
    "    elif trend == \"down\" and mom_ok and sig_ok and vol_ok:\n",
    "        signal_val = 0\n",
    "        rec_text   = f\"Sell {asset_symbol} @ {strike_price:.4f} ({expiry})\"\n",
    "    else:\n",
    "        signal_val = pd.NA\n",
    "        rec_text   = \"No trade\"\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"signal\"]          = signal_val\n",
    "    out[\"recommendation\"]  = rec_text\n",
    "    out[\"TrendValue\"]      = trend_val\n",
    "    out[\"MomentumValue\"]   = mom_val\n",
    "    out[\"SignalValue\"]     = sig_val\n",
    "    out[\"VolatilityValue\"] = vol_val\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d134b-d195-4273-a778-8307b115a7c5",
   "metadata": {},
   "source": [
    "**Define a function to run the backtesting pipeline**\n",
    "1. Get historical information\n",
    "2. Compute historical recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29acca55-7237-4dc0-8448-e92d3f730b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from typing import Union, Tuple, Optional, List\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "DateLike = Union[str, date]\n",
    "\n",
    "def get_date_range(\n",
    "    days_back: int,\n",
    "    start: Optional[DateLike] = None,\n",
    "    end: Optional[DateLike] = None,\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (start_date, end_date) as ISO strings.\n",
    "\n",
    "    If `start` is given, use it (parsed as ISO/string). Otherwise\n",
    "    compute start = end - days_back.\n",
    "    If `end` is given, use it; otherwise end = today.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    days_back : int\n",
    "        Number of days to go back when `start` is not provided.\n",
    "    start : str or date, optional\n",
    "        The explicit start date (inclusive). ISO format 'YYYY-MM-DD' or date.\n",
    "    end : str or date, optional\n",
    "        The explicit end date (inclusive). ISO format or date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (start_iso, end_iso) : Tuple[str, str]\n",
    "    \"\"\"\n",
    "    # Resolve end_date\n",
    "    if end is None:\n",
    "        end_date = date.today()\n",
    "    else:\n",
    "        end_date = (\n",
    "            datetime.fromisoformat(end).date()\n",
    "            if isinstance(end, str)\n",
    "            else end\n",
    "        )\n",
    "    # Resolve start_date\n",
    "    if start is not None:\n",
    "        start_date = (\n",
    "            datetime.fromisoformat(start).date()\n",
    "            if isinstance(start, str)\n",
    "            else start\n",
    "        )\n",
    "    else:\n",
    "        start_date = end_date - timedelta(days=days_back)\n",
    "\n",
    "    return start_date.isoformat(), end_date.isoformat()\n",
    "\n",
    "def run_backtest_pipeline(\n",
    "    client: Any,\n",
    "    s3_bucket: str,\n",
    "    s3_prefix: str,\n",
    "    tickers: List[str],\n",
    "    factors: TechnicalFactors,      \n",
    "    days_back: int = 60,\n",
    "    price_interval: str = \"1h\",\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    For each historical contract in S3, fetch price data up to 24h before\n",
    "    its expiration, compute indicators and a trade signal, then compare\n",
    "    the signal to the actual in-the-money flag. Returns a DataFrame with:\n",
    "      ticker, contract_dt, exp_dt, fetch_start, fetch_end,\n",
    "      strike_price, signal, in_the_money, actual\n",
    "    \"\"\"\n",
    "    # 1) Load your historical contracts\n",
    "    historical = load_recent_history(client, s3_bucket, s3_prefix, days_back)\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(\n",
    "        historical.iterrows(),\n",
    "        total=len(historical),\n",
    "        desc=\"Backtesting contracts\"\n",
    "    ):\n",
    "        ticker       = row[\"ticker\"]\n",
    "        strike_price = row[\"strike\"]\n",
    "        contract_dt  = row[\"dt\"]\n",
    "        exp_time_str = row[\"exp_time\"]\n",
    "\n",
    "        # 2) Build expiration datetime & 24h‐prior cutoff\n",
    "        exp_dt     = parse_expiration(contract_dt, exp_time_str)\n",
    "        fetch_end  = exp_dt - timedelta(hours=24)\n",
    "\n",
    "        # 3) Clamp lookback for sub-daily to max 60 days\n",
    "        lookback_days = days_back if price_interval == \"1d\" else min(days_back, 60)\n",
    "        fetch_start   = fetch_end - timedelta(days=lookback_days)\n",
    "\n",
    "        if fetch_start >= fetch_end:\n",
    "            print(f\"⚠️  Skipping {ticker} @ {contract_dt}: \"\n",
    "                  f\"fetch_start {fetch_start} ≥ fetch_end {fetch_end}\")\n",
    "            continue\n",
    "\n",
    "        # 4) Download price data (skip on any error, fully silencing yfinance)\n",
    "        try:\n",
    "            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "                price_df = fetch_price(\n",
    "                    ticker,\n",
    "                    start=fetch_start,   # datetime.date or datetime.datetime\n",
    "                    end=fetch_end,       # datetime.datetime\n",
    "                    interval=price_interval,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        if price_df.empty:\n",
    "            continue\n",
    "\n",
    "        # 5) Compute indicators\n",
    "        ind_df = (\n",
    "            price_df\n",
    "            .pipe(compute_macd, factors)\n",
    "            .pipe(compute_rsi, factors)\n",
    "            .pipe(compute_atr, factors)\n",
    "        )\n",
    "\n",
    "        if ind_df.empty:\n",
    "            print(f\"⚠️  {ticker} @ {contract_dt}: insufficient data for indicators → skipping\")\n",
    "            continue\n",
    "\n",
    "        # 6) Generate trade signal\n",
    "        sig_df = ind_df.pipe(\n",
    "            generate_trade_signal,\n",
    "            asset_symbol=ticker,\n",
    "            strike_price=strike_price,\n",
    "            factors=factors,\n",
    "            expiry=\"24h-prior\",\n",
    "        )\n",
    "        \n",
    "        # Extract the raw signal (might be pd.NA)\n",
    "        raw_signal = sig_df[\"signal\"].iloc[-1]\n",
    "        \n",
    "        # If it's NA, that's “No trade” → skip\n",
    "        if pd.isna(raw_signal):\n",
    "            continue\n",
    "        \n",
    "        # Otherwise safe to cast\n",
    "        signal = int(raw_signal)\n",
    "        \n",
    "        # 7) Compute actual outcome\n",
    "        actual = row[\"in_the_money\"] if signal == 1 else 1 - row[\"in_the_money\"]\n",
    "\n",
    "        results.append({\n",
    "            \"ticker\":       ticker,\n",
    "            \"contract_dt\":  contract_dt,\n",
    "            \"exp_dt\":       exp_dt,\n",
    "            \"fetch_start\":  fetch_start,\n",
    "            \"fetch_end\":    fetch_end,\n",
    "            \"strike_price\": strike_price,\n",
    "            \"signal\":       signal,\n",
    "            \"in_the_money\": row[\"in_the_money\"],\n",
    "            \"actual\":       actual,\n",
    "        })\n",
    "\n",
    "    # 8) Build summary DataFrame + print accuracy\n",
    "    out   = pd.DataFrame(results)\n",
    "    total = len(out)\n",
    "    wins  = int(out[\"actual\"].sum()) if total else 0\n",
    "    accuracy = wins / total if total else 0.0\n",
    "    \n",
    "    print(f\"✅ Backtest: {wins}/{total} correct → {accuracy:.1%}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa040c81-0339-4b31-9b25-02ddd52097f6",
   "metadata": {},
   "source": [
    "**Define a function to report on the results of the backtesting**\n",
    "1. Overall wins/ total/ accuracy\n",
    "2. Per Ticker wins/ total/ accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6cdf43e-a9c9-42ec-90db-a59ad4490ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def report_backtest_results(backtest_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints two tables:\n",
    "      1) Summary by Category (Overall/Buy/Sell)\n",
    "      2) Per‐Ticker with detailed counts & percentages\n",
    "\n",
    "    This function will automatically detect 'signal' (any case) and 'actual'\n",
    "    columns by lowercasing them internally, so you won’t get a KeyError.\n",
    "    \"\"\"\n",
    "    # — 1) Normalize column casing —\n",
    "    col_map = {}\n",
    "    for c in backtest_df.columns:\n",
    "        lc = c.lower()\n",
    "        if lc == \"signal\":\n",
    "            col_map[c] = \"signal\"\n",
    "        if lc == \"actual\":\n",
    "            col_map[c] = \"actual\"\n",
    "    df = backtest_df.rename(columns=col_map).copy()\n",
    "\n",
    "    # now fail fast if still missing\n",
    "    missing = [x for x in (\"signal\",\"actual\") if x not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"report_backtest_results needs columns {missing}; got {backtest_df.columns.tolist()}\")\n",
    "\n",
    "    # — 2) Drop no‐trade rows & ensure integer signals —\n",
    "    df = df.dropna(subset=[\"signal\"])\n",
    "    df[\"signal\"] = df[\"signal\"].astype(int)\n",
    "\n",
    "    # — helper to compute rec/win/pct —\n",
    "    def stats(sub: pd.DataFrame):\n",
    "        rec = len(sub)\n",
    "        win = int(sub[\"actual\"].sum())\n",
    "        pct = win / rec if rec else pd.NA\n",
    "        return rec, win, pct\n",
    "\n",
    "    # — 3) Build & print Summary table —\n",
    "    overall_rec, overall_win, overall_pct = stats(df)\n",
    "    buy_rec,     buy_win,     buy_pct     = stats(df[df.signal == 1])\n",
    "    sell_rec,    sell_win,    sell_pct    = stats(df[df.signal == 0])\n",
    "\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\"Category\":\"Overall\", \"Recommended\": overall_rec, \"Correct\": overall_win, \"Accuracy\": overall_pct},\n",
    "        {\"Category\":\"Buy\",     \"Recommended\":    buy_rec, \"Correct\":    buy_win,    \"Accuracy\": buy_pct},\n",
    "        {\"Category\":\"Sell\",    \"Recommended\":   sell_rec, \"Correct\":   sell_win,   \"Accuracy\": sell_pct},\n",
    "    ])\n",
    "\n",
    "    # human‐friendly rename + percentage formatting\n",
    "    summary_print = summary_df.rename(columns={\n",
    "        \"Recommended\": \"# Recommended\",\n",
    "        \"Correct\":     \"# Correct\",\n",
    "        \"Accuracy\":    \"% Correct\",\n",
    "    })\n",
    "    summary_print[\"% Correct\"] = summary_print[\"% Correct\"].map(\n",
    "        lambda x: f\"{x:.1%}\" if pd.notna(x) else pd.NA\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Summary by Category ===\")\n",
    "    print(tabulate(summary_print, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n",
    "\n",
    "    # — 4) Build & print Per‐Ticker table —\n",
    "    rows = []\n",
    "    for ticker, grp in df.groupby(\"ticker\"):\n",
    "        rec_buy,  win_buy,  pct_buy  = stats(grp[grp.signal == 1])\n",
    "        rec_sell, win_sell, pct_sell = stats(grp[grp.signal == 0])\n",
    "        rec_tot,  win_tot,  pct_tot  = stats(grp)\n",
    "\n",
    "        rows.append({\n",
    "            \"Ticker\":     ticker,\n",
    "            \"Rec_Total\":  rec_tot,  \"Corr_Total\": win_tot,  \"Pct_Total\": pct_tot,\n",
    "            \"Rec_Buy\":    rec_buy,  \"Corr_Buy\":  win_buy,  \"Pct_Buy\":   pct_buy,\n",
    "            \"Rec_Sell\":   rec_sell, \"Corr_Sell\": win_sell, \"Pct_Sell\":  pct_sell,\n",
    "        })\n",
    "\n",
    "    per_ticker_df = (\n",
    "        pd.DataFrame(rows)\n",
    "          .sort_values(\"Pct_Total\", ascending=False, na_position=\"last\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    per_ticker_print = per_ticker_df.rename(columns={\n",
    "        \"Rec_Total\":  \"Total Rec\",\n",
    "        \"Corr_Total\": \"Total Corr\",\n",
    "        \"Pct_Total\":  \"% Total\",\n",
    "        \"Rec_Buy\":    \"Buy Rec\",\n",
    "        \"Corr_Buy\":   \"Buy Corr\",\n",
    "        \"Pct_Buy\":    \"% Buy\",\n",
    "        \"Rec_Sell\":   \"Sell Rec\",\n",
    "        \"Corr_Sell\":  \"Sell Corr\",\n",
    "        \"Pct_Sell\":   \"% Sell\",\n",
    "    })\n",
    "    for pct_col in [\"% Total\", \"% Buy\", \"% Sell\"]:\n",
    "        per_ticker_print[pct_col] = per_ticker_print[pct_col].map(\n",
    "            lambda x: f\"{x:.1%}\" if pd.notna(x) else pd.NA\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== By Ticker (Detailed) ===\")\n",
    "    print(tabulate(per_ticker_print, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecae55b-1c13-4e7c-a664-40a985a5a5e2",
   "metadata": {},
   "source": [
    "**Define a function to create and upload a report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbedef9-b25e-4f7f-9bc6-2bd422458370",
   "metadata": {},
   "source": [
    "**Define a function to summarize all the backtests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c583fae-75a0-4c03-8ada-47760d6fa590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def summarize_variations(\n",
    "    variation_results: Dict[str, pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a dict mapping variation names to backtest DataFrames\n",
    "    (each with an 'actual' column of 1 for correct, 0 for incorrect),\n",
    "    return a summary DataFrame indexed by variation with columns:\n",
    "      - Recommended : total trades recommended\n",
    "      - Correct     : number of correct trades\n",
    "      - Accuracy    : Correct / Recommended (NaN if Recommended == 0)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for name, df in variation_results.items():\n",
    "        rec   = len(df)\n",
    "        corr  = int(df[\"actual\"].sum()) if rec else 0\n",
    "        acc   = corr / rec if rec else pd.NA\n",
    "        rows.append({\n",
    "            \"Variation\":  name,\n",
    "            \"Recommended\": rec,\n",
    "            \"Correct\":     corr,\n",
    "            \"Accuracy\":    acc,\n",
    "        })\n",
    "\n",
    "    summary = (\n",
    "        pd.DataFrame(rows)\n",
    "          .set_index(\"Variation\")\n",
    "          .sort_values(\"Accuracy\", ascending=False, na_position=\"last\")\n",
    "    )\n",
    "    # format Accuracy as percentage, leave NaN for zero-recommendation\n",
    "    summary[\"Accuracy\"] = summary[\"Accuracy\"].map(\n",
    "        lambda x: f\"{x:.1%}\" if pd.notna(x) else pd.NA\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058125d-2be1-4f16-bf7a-d1bcc7401493",
   "metadata": {},
   "source": [
    "**Run backtesting pipeine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e49477eb-8250-44e3-9fe4-46d2edca5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Running backtest: baseline ──\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9702ba3a6841cb92bfd0a92ed0768c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Backtesting contracts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Backtest: 0/0 correct → 0.0%\n",
      "DEBUG — backtest DF for baseline:\n",
      "  shape = (0, 0)\n",
      "  cols  = []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Results for baseline:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"report_backtest_results needs columns ['signal', 'actual']; got []\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# You can reuse your report helper:\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mreport_backtest_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m summarize_variations(all_results)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_df)\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mreport_backtest_results\u001b[0;34m(backtest_df)\u001b[0m\n\u001b[1;32m     24\u001b[0m missing \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_backtest_results needs columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbacktest_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# — 2) Drop no‐trade rows & ensure integer signals —\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: \"report_backtest_results needs columns ['signal', 'actual']; got []\""
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "from IPython.display import display\n",
    "\n",
    "TICKERS = {\n",
    "    'CL=F',\n",
    "    'ES=F',\n",
    "    'GC=F',\n",
    "    'NQ=F',\n",
    "    'RTY=F',\n",
    "    'YM=F',\n",
    "    'NG=F',\n",
    "    'AUDUSD=X',\n",
    "    'EURJPY=X',\n",
    "    'EURUSD=X',\n",
    "    'GBPJPY=X',\n",
    "    'GBPUSD=X',\n",
    "    'USDCAD=X',\n",
    "    'USDCHF=X',\n",
    "    'USDJPY=X'\n",
    "}\n",
    "\n",
    "factor_variations = {\n",
    "    \"baseline\": TechnicalFactors(),\n",
    "    \"fast_emas\": TechnicalFactors(ema_short=8, ema_long=20),\n",
    "    \"slow_emas\": TechnicalFactors(ema_short=20, ema_long=50),\n",
    "    # \"tight_vol\": TechnicalFactors(volatility_multiplier=0.3),\n",
    "}\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for name, factors in factor_variations.items():\n",
    "    print(f\"\\n── Running backtest: {name} ──\")\n",
    "    s3_client = boto3.client(\"s3\", region_name=\"us-east-1\")\n",
    "    df = run_backtest_pipeline(\n",
    "        client=s3_client,\n",
    "        s3_bucket=\"nadex-daily-results\",\n",
    "        s3_prefix=\"historical\",\n",
    "        tickers=TICKERS,\n",
    "        factors=factors,              # <— pass the variation\n",
    "        days_back=20,\n",
    "        price_interval=\"1d\",\n",
    "    )\n",
    "    all_results[name] = df\n",
    "\n",
    "    print(f\"DEBUG — backtest DF for {name}:\")\n",
    "    print(\"  shape =\", df.shape)\n",
    "    print(\"  cols  =\", df.columns.tolist())\n",
    "    print(df.head(3))\n",
    "\n",
    "\n",
    "    # You can reuse your report helper:\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    report_backtest_results(df)\n",
    "\n",
    "    summary_df = summarize_variations(all_results)\n",
    "    print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7800030-1328-49e6-8eb0-259ee8934189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
